---
title: "Overview January 2021"
author: Renata Diaz
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  github_document:
    toc: true
---

### Crossvalidaton


We had invoked cross validation to try and do model selection based on how well the model is able to predict withheld data. The goal there was to avoid overfitting, and particularly a tendency to overfit by assigning a different LDA topic to each timestep or each species. 


I set up a crossvalidation model selection pipeline where:

* Withhold a section of timestep(s) + a buffer
* Fit numerous LDA and TS combinations to the training data
* Test the fitted model's performance as its ability to predict the withheld observations
* Use many subsets per model specification and aggregate

There are some variants and details in the crossvalidation methods:

* A single LDA seed + number of topics, fit to slightly different subsets of a dataset, can lead to **very** different gamma and beta matrices. This makes it nonsensical to re-aggregate the models fit to the different subsets. I switched to subsetting the LDA **proportions** after they have been fit.



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
library(ggplot2)

ll_evals <- read.csv(here::here("old_but_mb_useful","all_evals_bbs_rtrg_1_11.csv")) %>%
  select(-X)

all_evals <- ll_evals %>%
  mutate(k = as.factor(k),
         seed = as.factor(seed),
         cpts = as.factor(cpts),
         cpts_seed_k = paste0(cpts, "_", seed, "_", k),
         cpts_k = paste0(k, "_", cpts))


all_evals_summary <- all_evals %>%
  group_by(k, seed, cpts, cpts_seed_k, cpts_k) %>%
  summarize(mean_ll = mean(loglik),
            upper_97_ll = quantile(loglik, probs = .975),
            lower_2_ll = quantile(loglik, probs = .025),
            n_infinite = sum(is.infinite(loglik))) %>%
  ungroup() %>%
  arrange(desc(mean_ll))

ggplot(all_evals, aes(x = k, y = loglik, group = cpts_seed_k, color = cpts)) +
 geom_boxplot() +
   theme_bw() +
   ggtitle("All models loglikelihood")


ggplot(all_evals_summary, aes(x = k, y = mean_ll, group = cpts_seed_k, color = cpts)) +
 geom_boxplot() +
   theme_bw() +
   ggtitle("All models MEAN loglikelihood")

```

When using cross validation, high-k models do indeed do poorly. However, it's hard to distinguish the number of changepoints, and large numbers of changepoints do very well. k here is the facet panels. 

```{r}
ggplot(all_evals_summary, aes(x = cpts, y = mean_ll, color = cpts)) +
  geom_point() +
facet_wrap(vars(k), nrow = 1) +
  theme_bw() +
  ggtitle("All models loglikelihood")
```

High numbers of changepoints win freuently:

```{r}
highest_low_95 = max(all_evals_summary$lower_2_ll)

all_evals_summary <- all_evals_summary %>%
  mutate(in_95 = upper_97_ll >= highest_low_95)

all_evals_summary[1:10, ]
```

And a lot of models, with different numbers of changepoints, do very well:

```{r}
ggplot(filter(all_evals, cpts_seed_k %in% filter(all_evals_summary, in_95)$cpts_seed_k), aes(x = k, y = loglik, group = cpts_seed_k, color = cpts)) +
  geom_boxplot() +
#  facet_wrap(vars(cpts)) +
  theme_bw() +
  ggtitle("Models w/LL overlapping top 95%")
```


Digging into this, we can see that the best fitting model (`k = 2, seed = 16, cpts = 5`) is not very different from a model with 1 changepoint, and I'd argue that the extra changepoints are not picking up on "important" dynamics. We're getting more changepoints than we want.

```{r}
source(here::here("analysis", "fxns", "crossval_fxns.R"))
load(here::here("old_but_mb_useful", "bbs_1_11.RData"))

dat <- bbs_rtrg_1_11

lda_8 <- LDATS::LDA_set_user_seeds(dat$abundance, topics = 2, seed = 16)

plot(lda_8)

ts_8 <- LDATS::TS_on_LDA(lda_8[[1]], as.data.frame(dat$covariates), formulas = ~1, nchangepoints = c(0, 1, 5), timename = "year", control = LDATS::TS_control(nit = 100))

for(i in 1:3){ 
gamma_plot(ts_8[[i]], selection = "mode")
}
```