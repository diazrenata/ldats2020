---
title: "Overview January 2021"
author: Renata Diaz
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  github_document:
    toc: true
    fig_width: 4
    fig_height: 3
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
library(ggplot2)
library(LDATS)
source(here::here("analysis", "fxns", "crossval_fxns.R"))
source(here::here("analysis", "fxns", "hybrid_fxns.R"))
source(here::here("analysis", "fxns", "make_toy_data_objects.R"))

```
# Overview

## Usefulness of the approach

I am now thinking of LDATS as a tool for detecting & describing temporal structure in timeseries of community composition. Community composition may be relatively static over time, or might shift gradually or rapidly between two or more transient states. A major challenge in detecting, let alone explaining or predicting, such shifts relates to the high dimensionality of most community data. The researcher has to choose whether to focus on a few abundant or apparent species, use a dimensionality reduction algorithm, or use a distance metric - neither of which approaches is inherently suited to temporal community data. LDATS accomplishes dimensionality reduction and temporal analysis, and (now) optimizes the dimensionality reduction to facilitate accurate, but parsimonious, description of specifically temporal dynamics. 

## Applied to BBS

We take a macroecological approach to detecting and describing shifts between community states for North American bird communities over the past 40 years. We ask: 

1. How common it is to have a) relatively little temporal structure, meaning static or temporally randomish dynamics, or b) transitions between multiple states? 
2. (Potentially) Do these transitions tend to occur rapidly or gradually?
3. Do transitions - in particular rapid ones - coincide with periods of overall low abundance? 

Beyond the scope here, this method could also be useful combined with other data streams & community-specific hypotheses to ask:

* Which species are responsible for the change
* What endogenous or exogenous factors coincide with periods of change
    * environmental shifts; key species crashing out; etc
* If there are regional or national patterns in if or when transitions occur
    * e.g. New England is static but there are multiple states for communities from the Southwest
    * or, communities nationwide underwent a shift between 1990-95

# Technical details
 
* Following Christensen et al (and in a change from past years in MATSS-LDATS), we do not try to fit temporal dynamics - i.e. slope - within a time chunk. This is because fitting slopes allows the model to fit even very rapid, changepoint like dynamics within a single chunk. All models are fit as intercept-only within chunks. (`formula = response ~ 1`)
* Also following C., we may be able to use the (un)certainty of the changepoint model's estimates of when the changepoints occur to infer how rapidly or gradually a transition took place. I am still testing this for coarser (40-sample v 400-sample) datasets.
* We proceed using the softmax transformation. **Juniper** - is this acceptable (even if not ideal)? My impression was that the softmax was most problematic when we were fitting slopes, but you're the expert on these details. 


## Model selection

 **I want to give everyone, and especially technically-inclined folks, the chance to investigate this, ask any questions, and raise any concerns.**

Here in brief; see technical_details.Rmd for details and figures.

The original LDATS implementation first selects an LDA via AIC and then selects a TS model via AIC. This tends to select an LDA with a lot of topics. The TS model then struggles to fit anything to the (very high dimensional) topic proportions. 

I tried a combined/holistic selection process using leave-one-out crossvalidation. That is, we try all possible combinations of numbers of topics and numbers of changepoints, and pick the best *combination* of number of topics and number of changepoints according to how well it predicts withheld data. 

Crossvalidation effectively curbs the tendency to have too many topics, but tends to fit a lot of *changepoints*. We did not have this problem when we selected the changepoints with AIC, because AIC penalizes for additional parameters. A changepoint has to improve the fit so much that it is "worth" the parameter penalty. However, with crossvalidation, there is no penalty for parameters directly. Overly-complex models are supposed to be selected against because they will be overfit and will do a bad job predicting test data. The problem is that having an extra changepoint doesn't necessarily make a model so complex it is bad at prediction - the change associated with the changepoint can be minimal or nonexistent. This means extra changepoints can sometimes make the fit a little better but rarely make it a lot worse.

My solution is to select the number of changepoints via AIC, so we get the parameter penalty, and select the number of topics via crossvalidation, so we get a topic structure that helps us capture the gist of the whole community dynamics via a simple TS model. This is tricky because 1) we can't use AIC to compare TS models fit to different LDAs, and 2) we need the TS fit in order to do crossvalidation on the LDA. 

My approach is to restrict the models considered via crossvalidation to the best-scoring TS model for each particular LDA fit. That is, for a particular LDA (2 topics, seed = 100), we fit a bunch of TS models (0, 1, 2, 5 changepoints). We compare the TS models via AIC, and enter *only the best-fitting TS model for each LDA* into crossvalidation. We then use crossvalidation to pick the LDA model that, when combined with its best-fitting TS model, allows for the best performance at predicting withheld data. 

That is, we might be comparing "2 topics, seed = 100, 2 changepoints" with "5 topics, seed = 10, 0 changepoints", and finding that the 2 topic LDA, with 2 changepoints, does a better job recovering the actual species abundances than a 5 topic LDA with no changepoints. 

This seems to work. However, I would not have done it this way if not for the string of issues discussed above. It's unconventional!


# Sample results

## What changepoints mean now

The presence & number of changepoints tells us how many community states the model is using to describe the data and when the system changed from one state to another. 

The presence of a changepoint means there was a transition, but not necessarily a *rapid* one. The uncertainty around *when* the changepoint occurred may reflect how rapid it was.

A transition doesn't need to be a total overhaul of the community/"regime shift" for the model to find a changepoint. It has to be reasonably substantial and consistent, but just flagging that transitions between states != regime shift. 

## What topics mean now

In this application, the number of topics and the species composition of the topics is less intuitively informative than in the Portal application. Here, the topic structure doesn't seem to be picking up on "functional" community types with fine variation in their relative proportions over time. Rather, we tend to see one topic corresponding to the community state in one time period, and, if there is a transition to another community state, another topic post-transition. 

<!-- I think this is because of two related things. One, we have fewer time samples, so less capacity to detect fine scale dynamics. Two, we specifically look for the set of topics that allows the change point model to achieve a good fit, which means the topics need to have relatively simple temporal dynamics.  -->

## Portal, reduced to annual samples

(I am not doing monthly Portal because of the seasonal signal.)

```{r}

all_evals <- read.csv(here::here("analysis", "all_evals_hybrid_portal.csv")) %>%
  mutate(k_seed = paste(k, seed, sep = "_"))

all_evals_summary <- all_evals %>%
  group_by(dataset, k, seed, cpts) %>%
  summarize(mean_sum_ll = mean(sum_loglik)) %>%
  arrange(desc(mean_sum_ll)) %>%
  group_by(dataset) %>%
  mutate(dat_rank = row_number())


ggplot(filter(all_evals_summary), aes(as.factor(k), mean_sum_ll, color = as.factor(cpts))) + 
  geom_point() + 
  facet_wrap(vars(dataset), scales = "free_y")

# Some but not all have clear winners. 
head(filter(all_evals_summary, dat_rank < 6))


#### load datasets ####
rats <- get_toy_data("rodents_annual",  here::here("analysis", "toy_datasets"))
rats$covariates$year <- 1:nrow(rats$covariates)
```


```{r static changepoint}

lda_rats<- LDATS::LDA_set_user_seeds(rats$abundance, topics = 2, seed = 6)

plot_lda_comp(lda_rats, specl = TRUE)
plot_lda_year(lda_rats, rats$covariates$year)
ts_rats <- LDATS::TS_on_LDA(lda_rats, as.data.frame(rats$covariates), formulas = ~1, nchangepoints = 1, timename = "year", control = LDATS::TS_control(nit = 100))

gamma_plot(ts_rats[[1]])
rho_plot(ts_rats[[1]]) 
```



## A BBS route

```{r}

all_evals <- read.csv(here::here("analysis", "all_evals_hybrid.csv"))

all_evals <- all_evals %>%
  mutate(k_seed = paste(k,seed, sep = "_")) %>%
  filter(grepl(dataset, pattern = "bbs_rtrg_1_11"))
  
all_evals_summary <- all_evals %>%
  group_by(dataset, k, seed, cpts) %>%
  summarize(mean_sum_ll = mean(sum_loglik)) %>%
  arrange(desc(mean_sum_ll)) %>%
  group_by(dataset) %>%
  mutate(dat_rank = row_number())


ggplot(filter(all_evals_summary), aes(as.factor(k), mean_sum_ll, color = as.factor(cpts))) + 
  geom_point() + 
  facet_wrap(vars(dataset), scales = "free_y")

# Some but not all have clear winners. 
all_evals_summary[1:10,]

```

```{r}
load(here::here("analysis", "reports", "hybrid_datasets.RData"))

long_1_11 <- bbs_rtrg_1_11$abundance %>%
  mutate(year = bbs_rtrg_1_11$covariates$year) %>%
  tidyr::pivot_longer(-year, names_to = "species", values_to = "abundance")

ggplot(long_1_11, aes(year, abundance, color = species)) +
  geom_line() +
  theme_bw() +
  theme(legend.position = "none")

lda_1_11 <- LDATS::LDA_set_user_seeds(bbs_rtrg_1_11$abundance, topics = 3, seed = 8)
ts_1_11 <- LDATS::TS_on_LDA(lda_1_11[[1]], as.data.frame(bbs_rtrg_1_11$covariates), formulas = ~1, nchangepoints = 2, timename = "year", control = TS_control(nit = 100))
plot_lda_comp(lda_1_11)
plot_lda_year(lda_1_11, bbs_rtrg_1_11$covariates$year)
gamma_plot(ts_1_11[[1]])
rho_plot(ts_1_11[[1]])
```